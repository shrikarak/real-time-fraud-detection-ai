{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Real-Time Credit Card Fraud Detection\n",
    "\n",
    "**Copyright (c) 2026 Shrikara Kaudambady. All rights reserved.**\n",
    "\n",
    "This notebook implements a machine learning model to detect fraudulent credit card transactions. The key challenge in this problem is the **severe class imbalance**â€”fraudulent transactions are extremely rare. We will address this by using the **SMOTE (Synthetic Minority Over-sampling Technique)** and evaluate our model using metrics appropriate for imbalanced data, such as the Area Under the Precision-Recall Curve (AUPRC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Explore the Data\n",
    "We will use a standard, anonymized credit card fraud dataset. The first step is to load the data and examine the class distribution to understand the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a public source\n",
    "df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nClass Distribution:\")\n",
    "class_dist = df['Class'].value_counts()\n",
    "print(class_dist)\n",
    "print(f\"\\nProportion of Fraudulent Transactions: {class_dist[1] / class_dist[0] * 100:.4f}%\")\n",
    "\n",
    "# Visualize the imbalance\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title('Class Distribution (0: Legitimate, 1: Fraud)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "The features in this dataset are already anonymized via PCA, but the `Time` and `Amount` columns are not. We should scale these columns to prevent them from disproportionately influencing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "df['scaled_Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df['scaled_Time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "\n",
    "df_processed = df.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "print(\"Data scaled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handling Class Imbalance with SMOTE\n",
    "This is the most critical step. We first split our data into training and testing sets. Then, we apply SMOTE *only to the training set* to create a balanced dataset for the model to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop('Class', axis=1)\n",
    "y = df_processed['Class']\n",
    "\n",
    "# Split data BEFORE applying SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Original training set shape:\", X_train.shape)\n",
    "print(\"Original training set fraud count:\", y_train.sum())\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "print(\"\\nApplying SMOTE to the training data...\")\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nResampled training set shape:\", X_train_resampled.shape)\n",
    "print(\"Resampled training set class distribution:\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Classification Model\n",
    "We'll use a LightGBM classifier, which is a fast and powerful gradient boosting model, trained on our new, balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "\n",
    "print(\"Training LightGBM model...\")\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Evaluation\n",
    "We evaluate the model on the original, imbalanced test set, as this represents the real-world scenario. We will focus on precision, recall, and the area under the precision-recall curve (AUPRC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "# Calculate Precision-Recall curve and AUPRC\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "auprc = auc(recall, precision)\n",
    "print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc:.4f}\")\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(recall, precision, label=f'LightGBM (AUPRC = {auprc:.4f})')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Legitimate', 'Fraud'], yticklabels=['Legitimate', 'Fraud'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}